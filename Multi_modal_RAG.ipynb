{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59df23c-86f7-4e5d-8b8c-de92a92f6637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    return partition_pdf(fname,\n",
    "                       extract_images_in_pdf=True,\n",
    "                       infer_table_structure=True,\n",
    "                       chunking_strategy=\"title\",\n",
    "                        max_characters=4000,\n",
    "                        new_after_n_chars=3800,\n",
    "                        combine_text_under_n_chars=2000\n",
    "    )\n",
    "\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# File path\n",
    "fpath = \"/Users/sungajiwala/Desktop/KP Sanghvi/Coding Projects/langchain/Attention_is_all_you_need.pdf\"\n",
    "fname = \"Attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# Get text, tables\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523e6ed2-2132-4748-bdb7-db765f20648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6b1d97-4245-45ac-95ba-9bc1cfd10182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 8/8 [01:07<00:00,  8.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=200)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the raw image. \\\n",
    "        Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for img_file in tqdm(sorted(os.listdir(path)), desc=\"Processing images\"):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "\n",
    "            try:\n",
    "                base64_image = encode_image(img_path)\n",
    "                img_base64_list.append(base64_image)\n",
    "                image_summaries.append(image_summarize(base64_image, prompt))\n",
    "                count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_file}: {e}\")\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(\"figures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a0a289-b970-49fe-b04f-5d857a4c159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-storage\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a deep learning and machine learning specialist.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts.\\n\"\n",
    "            \"Use this information to provide quality information related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f4695c6-7374-4284-b2fe-a94ac17b630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check retrieval\n",
    "query = \"Can you give me a brief description on the document.\"\n",
    "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c64b19e-5a89-4dda-af38-fcc4a36a1b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images you've provided appear to be visual representations of text data, possibly from a natural language processing (NLP) task or a text visualization tool\n",
      " These types of visualizations are often used to analyze the structure of sentences, the relationships between words, or the frequency of word usage\n",
      "\n",
      "\n",
      "Each image shows lines connecting words, which could represent syntactic relationships or the flow of a sentence\n",
      " The colors and thickness of the lines might indicate different types of relationships or the strength of connections between words\n",
      " The words \"<EOS>\" and \"<pad>\" are commonly used in NLP and stand for \"End Of Sentence\" and \"padding\" respectively\n",
      " Padding is used to fill in space in data to ensure consistent lengths of sequences for processing\n",
      "\n",
      "\n",
      "The first image seems to be discussing the passage of new laws by American governments since 2009 that have made the voting or registration process more difficult\n",
      "\n",
      "\n",
      "The second, third, and fourth images seem to be variations of the same sentence, \"The law will never be perfect, but its application should be just\n",
      "\" The different colors could represent different analyses or interpretations of the sentence structure\n",
      "\n",
      "\n",
      "Without more context, it's difficult to provide a more detailed description, but generally, these images are used in data analysis to understand text structure, sentiment, or to build and debug models that process language\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "response = chain_multimodal_rag.invoke(query)\n",
    "response = response.split('.')\n",
    "\n",
    "# Print each line in a new line\n",
    "for line in response:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846ea682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism is a concept in the field of deep learning, particularly within the context of neural networks that process sequential data, such as natural language processing (NLP) tasks\n",
      " It allows a model to focus on different parts of the input sequence when predicting each part of the output sequence, much like how humans pay attention to different aspects of a scene or conversation to understand it better\n",
      "\n",
      "\n",
      "In the context of the images you've provided, the attention mechanism is illustrated as part of the architecture known as the Transformer, which is widely used in NLP\n",
      " The Transformer uses a specific type of attention called \"Scaled Dot-Product Attention\" and an extension of it called \"Multi-Head Attention\n",
      "\"\n",
      "\n",
      "Here's a breakdown of the components shown in the images:\n",
      "\n",
      "1\n",
      " **Scaled Dot-Product Attention**: This is the core of the attention mechanism in the Transformer model\n",
      " It takes three inputs: queries (Q), keys (K), and values (V)\n",
      " The attention scores are computed by taking the dot product of the queries with the keys, scaling those scores by the dimensionality of the keys (to avoid extremely large values), and then applying a softmax function to obtain weights on the values\n",
      " Optionally, a mask can be applied to the scores before softmax to prevent the model from attending to certain positions (useful for tasks like language modeling where future information should not influence the current prediction)\n",
      "\n",
      "\n",
      "2\n",
      " **Multi-Head Attention**: This is an extension of the scaled dot-product attention\n",
      " Instead of performing attention once, the model does it multiple times in parallel (the heads), each time with different, learned linear projections of the queries, keys, and values\n",
      " The outputs of these parallel attention processes are then concatenated and once again linearly transformed\n",
      " This allows the model to jointly attend to information from different representation subspaces at different positions\n",
      "\n",
      "\n",
      "The table you provided compares the complexity and maximum path length of different layer types, including self-attention (as used in the Transformer), recurrent (as used in RNNs and LSTMs), and convolutional layers\n",
      " The self-attention mechanism has a complexity that scales with the square of the sequence length (n) and the dimensionality of the representations (d), but it allows for constant maximum path length operations, which means that each output element can be computed in parallel and has direct access to all input elements\n",
      " This is beneficial for learning long-range dependencies in the data\n",
      "\n",
      "\n",
      "In summary, the attention mechanism is a powerful tool in deep learning that enables models to dynamically focus on relevant parts of the input to make better predictions, and it has been particularly transformative in the field of NLP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is attetion mechanism?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98e45a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset used for training the parsers and models mentioned in the provided text is the Wall Street Journal (WSJ) portion of the Penn Treebank\n",
      " This is indicated by the repeated mention of \"WSJ only\" in the context of the training for various models and approaches, such as those by Vinyals & Kaiser et al\n",
      " (2014), Petrov et al\n",
      " (2006), Zhu et al\n",
      " (2013), and others\n",
      "\n",
      "\n",
      "The Wall Street Journal dataset is a widely used corpus for training and evaluating natural language processing models, particularly for tasks such as part-of-speech tagging, syntactic parsing, and other linguistic analyses\n",
      " It contains a large collection of annotated text from news articles, which provides a standard benchmark for comparing the performance of different models and approaches in the field of computational linguistics and machine learning\n",
      "\n",
      "\n",
      "Additionally, the text mentions different configurations and results for the Transformer model, including a \"base\" and a \"big\" version, with various hyperparameters such as the number of layers (N), model dimension (dmodel), feed-forward dimension (dff), number of attention heads (h), and others\n",
      " These configurations are used to train the models and evaluate their performance on tasks like machine translation, as indicated by the BLEU scores for English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What dataset was for used for training?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17e7ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the ByteNet model is as follows:\n",
      "\n",
      "- For English to German (EN-DE): 23\n",
      "75\n",
      "- For English to French (EN-FR): 24\n",
      "6\n",
      "\n",
      "These scores are a measure of the quality of the machine-translated text compared to a set of high-quality reference translations\n",
      " The BLEU score ranges from 0 to 100, where higher scores indicate better translations that are closer to the quality of a human translator\n",
      " The ByteNet model's scores are part of a comparison with other models, and you can see that there are models with higher BLEU scores, such as the Transformer models, which achieve scores over 40 in both language pairs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the BLEU score for ByteNet model?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74c9223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the ConvS2S (Convolutional Sequence to Sequence) model is as follows:\n",
      "\n",
      "- For English to German (EN-DE): 25\n",
      "16\n",
      "- For English to French (EN-FR): 26\n",
      "36\n",
      "\n",
      "These scores are for the single model, not the ensemble\n",
      " The ensemble versions of the ConvS2S model have higher BLEU scores:\n",
      "\n",
      "- For English to German (EN-DE): 26\n",
      "36\n",
      "- For English to French (EN-FR): 41\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the BLEU score for ConvS2S model?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5712dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The part of the document that contains all the BLEU scores for different models is in the text section under the heading \"BLEU EN-DE EN-FR\n",
      "\" Here you can find the BLEU scores for various models when translating between English to German (EN-DE) and English to French (EN-FR)\n",
      " The models listed are ByteNet, Deep-Att + PosUnk, GNMT + RL, ConvS2S, MoE, and their respective ensemble versions, as well as the Transformer base and big models\n",
      " The BLEU scores are listed in two columns, one for each language pair\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What part of the documnet has all the BLEU score for different models?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
