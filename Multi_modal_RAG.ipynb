{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59df23c-86f7-4e5d-8b8c-de92a92f6637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    return partition_pdf(fname,\n",
    "                       extract_images_in_pdf=True,\n",
    "                       infer_table_structure=True,\n",
    "                       chunking_strategy=\"title\",\n",
    "                        max_characters=4000,\n",
    "                        new_after_n_chars=3800,\n",
    "                        combine_text_under_n_chars=2000\n",
    "    )\n",
    "\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# File path\n",
    "fpath = \"/Users/sungajiwala/Desktop/KP Sanghvi/Coding Projects/langchain/New-Creta-Brochure.pdf\"\n",
    "fname = \"New-Creta-Brochure.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# Get text, tables\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "523e6ed2-2132-4748-bdb7-db765f20648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6b1d97-4245-45ac-95ba-9bc1cfd10182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 102/102 [07:33<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=200)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the raw image. \\\n",
    "        Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for img_file in tqdm(sorted(os.listdir(path)), desc=\"Processing images\"):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "\n",
    "            try:\n",
    "                base64_image = encode_image(img_path)\n",
    "                img_base64_list.append(base64_image)\n",
    "                image_summaries.append(image_summarize(base64_image, prompt))\n",
    "                count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_file}: {e}\")\n",
    "                print(\"Waiting for 60 seconds before continuing...\")\n",
    "                time.sleep(60)  # Wait for 60 seconds\n",
    "\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(\"figures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a0a289-b970-49fe-b04f-5d857a4c159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-storage\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a car dealer and salesperson.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts.\\n\"\n",
    "            \"Use this information to provide quality advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f4695c6-7374-4284-b2fe-a94ac17b630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check retrieval\n",
    "query = \"Can you give me a brief description on the document.\"\n",
    "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c64b19e-5a89-4dda-af38-fcc4a36a1b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be a brochure or specification sheet for a Hyundai vehicle, possibly a model lineup that includes different variants of a car\n",
      " The first two images contain detailed tables listing various features and specifications for different trim levels of the vehicle\n",
      " These tables compare the features available in each variant, such as engine options, interior features, safety equipment, wheels, lighting, and infotainment systems\n",
      "\n",
      "\n",
      "The variants listed include \"E,\" \"EX,\" \"S,\" \"SX,\" and \"SX(O),\" with each subsequent variant typically offering more features than the previous\n",
      " The document outlines the technical specifications for three engine types: a 1\n",
      "5L MPi petrol engine, a 1\n",
      "5L U2 CRDi diesel engine, and a 1\n",
      "5L Turbo GDi petrol engine\n",
      " It also provides information on dimensions, transmission types, suspension, brakes, and tire sizes\n",
      "\n",
      "\n",
      "The third image shows a Hyundai vehicle driving on a scenic road with mountains in the background, which is likely part of the brochure's visual appeal to showcase the vehicle in an attractive setting\n",
      "\n",
      "\n",
      "The last image is a small, low-resolution picture of a blue car, which seems to be part of the brochure but is not clear enough to provide specific details about the model or its features\n",
      "\n",
      "\n",
      "Overall, the document is designed to inform potential buyers about the options and features available for a particular Hyundai car model, allowing them to compare different variants and make an informed decision based on their preferences and needs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "response = chain_multimodal_rag.invoke(query)\n",
    "response = response.split('.')\n",
    "\n",
    "# Print each line in a new line\n",
    "for line in response:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846ea682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided images, which appear to be specification sheets for different variants of a vehicle, I can provide a comparison between the top model and the one directly below it in terms of specifications\n",
      " However, the images are not entirely clear, so I will provide a general comparison based on typical differences between top models and lower-tier models in a vehicle lineup\n",
      "\n",
      "\n",
      "**Engine & Performance:**\n",
      "- The top model often comes with a more powerful engine option, such as a turbocharged variant, offering higher horsepower and torque compared to the model below it\n",
      "\n",
      "- Transmission options may differ, with the top model possibly offering an advanced transmission like a dual-clutch transmission (DCT) for smoother and faster gear shifts\n",
      "\n",
      "\n",
      "**Exterior Features:**\n",
      "- The top model may have additional exterior features such as LED headlights, larger wheels, and more elaborate design elements like chrome accents or a sunroof\n",
      "\n",
      "- It might also include advanced safety features like parking assist systems, adaptive headlights, or a more comprehensive camera system\n",
      "\n",
      "\n",
      "**Interior Features:**\n",
      "- Inside, the top model typically includes higher-quality materials, leather upholstery, power-adjustable seats with memory function, and possibly a larger infotainment display with more connectivity options\n",
      "\n",
      "- Additional convenience features like a premium sound system, dual-zone climate control, and advanced driver-assist systems might be standard on the top model\n",
      "\n",
      "\n",
      "**Technology & Safety:**\n",
      "- The top model is likely to have a more advanced suite of safety and driver-assistance technologies, such as adaptive cruise control, lane-keeping assist, and collision avoidance systems\n",
      "\n",
      "- Infotainment and connectivity features are usually more comprehensive in the top model, including navigation, premium audio systems, and integration with mobile apps\n",
      "\n",
      "\n",
      "**Wheels & Tires:**\n",
      "- The top model may come with larger wheels and tires that offer better performance and aesthetics\n",
      "\n",
      "\n",
      "**Suspension & Brakes:**\n",
      "- There might be differences in the suspension tuning, with the top model offering a sportier or more adaptive setup\n",
      "\n",
      "- The braking system could be upgraded in the top model for better stopping power\n",
      "\n",
      "\n",
      "To provide a precise answer, I would need to be able to read the specific details from the images\n",
      " If you can provide clearer images or specify which models you are comparing, I can give you a more detailed and accurate comparison\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the difference between tht top model and the one below it when talking about specifications be precise with your answers?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98e45a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the images provided, it appears that the cars are indeed equipped with Lane Departure Warning (LDW). The first image clearly labels the feature as \"Lane Departure Warning (LDW)\" and shows a visual representation of how the system alerts the driver when the vehicle begins to move out of its lane without a turn signal being activated.\\n\\nIf you have any more specific questions about the LDW system or other safety features, feel free to ask!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_multimodal_rag.invoke(\"Are the cars equipped with LDW?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17e7ccfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lane Departure Warning (LDW) is a safety feature in modern vehicles designed to alert the driver when the vehicle begins to move out of its lane on freeways and arterial roads. This system uses cameras and sensors to monitor lane markings on the road surface, and if the vehicle detects that it is unintentionally drifting away from the lane without the turn signals being activated, it will warn the driver. This warning can come in various forms, such as a visual alert on the dashboard, an audible beep, a vibration in the steering wheel, or a combination of these signals.\\n\\nThe purpose of LDW is to prevent accidents caused by drivers who may be distracted, drowsy, or otherwise not paying full attention to the road. It's important to note that LDW systems do not take control of the vehicle; they simply provide a warning to the driver. It's still the driver's responsibility to take corrective action to safely maintain the correct lane position.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_multimodal_rag.invoke(\"What actually is LDW?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
